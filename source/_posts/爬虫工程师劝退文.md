---
title: 爬虫工程师劝退文
date: 2018-03-28 17:20:01
tags:
- 爬虫
categories:
- 经验总结
---
**前言：本文无源码，来聊聊爬虫工程师终将逝去的青春**

此刻的我正坐在工位上，看着面前的某网站爬虫代码，陷入了回忆。

#### 缘起
> “你写一下XX网的爬虫，主要是爬一下个人资料，你大概需要多久？”
“4、5天吧”，曾经用2、3天写了知乎爬虫的我给自己留了宽裕的时间用来摸鱼。
“那好，我给你一个星期时间。”
“好的”

这是零工作经验的我入职这家公司以来接到的第一个项目，从零开始写一个爬虫。

既不是在爬虫系统上添加功能，也不是给祖传代码填坑。

我给了自己4、5天时间。

然后就走上了这条不归路……

![](https://upload-images.jianshu.io/upload_images/5588611-1b5056ccd0b5570b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 一个爬(fei)虫(pin)的诞生
基于程序员必备的搜索技能，我迅速的在github上找到了一个相似的爬虫代码。

![](https://upload-images.jianshu.io/upload_images/5588611-8d5f0a633416105d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

修修改改，我只花费了2天时间就写好了爬虫，**post请求呀、ajax异步加载呀、正则匹配呀、json呀**什么的，甚至还优化了原作者的代码。

同时，距离作者写出这个代码，已经过去很久了，网站一些信息的获取规则已经不同了。由于网站特别的策略，我学会了**识别网页源码**伪装通过二次传递值来查找目标信息。(首先找到传递值，再通过传递值匹配目标信息)

于是，很快，我的爬虫欢乐地运行了起来。

并且，满(nei)怀(liu)热(man)情(mian)地花了1个多月时间为自己的爬虫补坑(手动再见。

#### 第一个障碍
爬虫跑起来之后，由于单个账号请求次数过多，出现了验证码，会往注册的邮箱里发一条包含验证码的邮件，填上就可以重新登入。

我始终没有通过requests越过这道坎，最终我选择了用**selenium**来登录网站并提交验证码，并将登录成功后的cookies信息传递给获取信息的函数块中。

这时，我学会了用**imaplib**模块登录邮箱获取验证码。

![](https://upload-images.jianshu.io/upload_images/5588611-0e9bae386e25d0e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### 部署服务器
解决了验证码登录的问题，程序又一次跑了起来，此时就需要部署到服务器上了。

selenium的浏览器chrome不利于在服务器上运行，于是我学会了将selenium结合无头浏览器**phantomjs**使用。

服务器是新开的，环境还需要配置，于是我又学会了**配置服务器的基本操作**。

![](https://upload-images.jianshu.io/upload_images/5588611-31bffe8d195069d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 功能分离

当单个账号单日发送的请求过多达到阀值时，网站毫不留情地封掉了账号，没有冷却时间，永久封禁。

同事为我提供了批量的账号，而我 跑崩了一个又一个。

于是我又改变了使用账号的策略，多个账号，轮流爬，每个爬几下就换。

此时不停地用selenium登录账号已经变成肥肠影响爬虫速度的一个事情了，于是我开始分割代码。

一个代码用来为账号更新cookie，一个代码直接取cookie，而可用的cookie信息，我存在了mysql中(这或许就是菜鸡吧)。。。。。

![](https://upload-images.jianshu.io/upload_images/5588611-7b48492a2b3a6c34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而这样的功能分离，它有一个高级的名字，**生产者-消费者模型**。

#### 多线程
此时爬虫，已经可以无障碍的运行了，挡在面前的问题是爬虫的速度，我开启了多线程，这时问题又来了，url的重复提取问题，单例运行时不需要考虑的问题，但当开启了多线程、多进程等并发操作时，url的去重就变得必要了。

再一次功能分离，将需要提取的url，放入redis库中，爬虫程序中直接使用pop函数获取url，取出即删除，不会出现重复提取的情况，**redis**是一个在缓存方面非常优秀的数据库。

此时的代码为了多线程的配合，将所有的与爬虫无关的，获取cookies，存取数据信息，都分离了出来，所有的通信都只存在于redis之间。

<!-- ```mermaid
graph TD
A(mysql) --> |提供账号|B[cookies_producer.py]
A(mysql) --> |提供源链接|D[link_producer.py]
B --> |产出cookies| E(redis)
D --> |产生链接| E(redis)
E --> |提供cookie| G[info_spider.py]
E --> |提供url| G
G --> |产生目标信息|H(mysql)
```
```mermaid
sequenceDiagram
  生产者mysql ->> 缓存redis: 将mysql中链接url放入redis中
  生产者mysql ->> 缓存redis: 从mysql取出cookies放入redis
  缓存redis ->> 消费者spider: 取出url进行爬取
  缓存redis ->> 消费者spider: 从redis取出cookies
  消费者spider ->> 缓存redis: 将爬去结果存入redis
  缓存redis ->> 生产者mysql: 将爬去结果存入redis
``` -->

![](https://upload-images.jianshu.io/upload_images/5588611-4806a6a26d170bbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/5588611-174c45f646ce555f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 最后
有人会想，这不就是一个项目的流程吗？

为什么这篇文章的名字是爬虫工程师劝退文呢？

因为我的账号依然被禁了。。。。。

于是我接手了同事的注册账号的项目。。。。。

砸了N多经费去买ip，买收验证码的手机号。。。。。

折腾了N久终于准备开心地把注册机代码也部署到服务器上形成程序闭环。。。。。
。
。
。
。
。
发现网站改了注册策略。。。。。。。

![](https://upload-images.jianshu.io/upload_images/5588611-ae67c10af9ae5dcc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**总结语**：你看，学了这么多知识，依然过不好这爬虫工程师的一生，大家还是不要私信问我需要哪些技能才能做爬虫工程师了！赶紧转行跑路吧！(敲黑板！划重点！)
